{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Caption Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "454e2164f7b14b4c92caa3738264d035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c081973025b4bfca089dad97eb0696f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e5a3f11d7df2472392fc019f6c77f6bd",
              "IPY_MODEL_2a975c4a4d3f4362805f48b62e2eb70a"
            ]
          }
        },
        "7c081973025b4bfca089dad97eb0696f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5a3f11d7df2472392fc019f6c77f6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c916d4b7241f4c5584070df58228ec30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d91186cc97314d1aabc61379c2845486"
          }
        },
        "2a975c4a4d3f4362805f48b62e2eb70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_487a45e56ff84be3b0f4815dd86cf2e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99165f85dea243c88d7ef0b0225d96a8"
          }
        },
        "c916d4b7241f4c5584070df58228ec30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d91186cc97314d1aabc61379c2845486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "487a45e56ff84be3b0f4815dd86cf2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99165f85dea243c88d7ef0b0225d96a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmeoLg-wgRmT",
        "colab_type": "code",
        "outputId": "ee3ac7ba-bbae-4ce8-91f1-05e5c53b1c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "454e2164f7b14b4c92caa3738264d035",
            "7c081973025b4bfca089dad97eb0696f",
            "e5a3f11d7df2472392fc019f6c77f6bd",
            "2a975c4a4d3f4362805f48b62e2eb70a",
            "c916d4b7241f4c5584070df58228ec30",
            "d91186cc97314d1aabc61379c2845486",
            "487a45e56ff84be3b0f4815dd86cf2e1",
            "99165f85dea243c88d7ef0b0225d96a8"
          ]
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import string\n",
        "from pickle import load,dump\n",
        "\n",
        "\n",
        "\n",
        "from keras.applications.xception import Xception,preprocess_input\n",
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model,load_model\n",
        "from keras.layers import Dense,Flatten,Input,LSTM,Dropout,Embedding\n",
        "from keras.layers.merge import add\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "454e2164f7b14b4c92caa3738264d035",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOJLOGX4Ua_t",
        "colab_type": "text"
      },
      "source": [
        "Code to load text file into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZeKuGieUi2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "  file=open(filename,'r')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1qdzPUoWfmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def all_image_caption(filename):\n",
        "  file=load_doc(filename)\n",
        "  captions=file.split('\\n')\n",
        "  descriptions={}\n",
        "  for caption in captions[:-1]:\n",
        "    img,caption=caption.split('\\t')\n",
        "    if img[:-2] not in descriptions:\n",
        "      descriptions[img[:-2]]\n",
        "    else:\n",
        "      decriptions.append[caption]\n",
        "  return descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuNJLxiAanaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_text(captions):\n",
        "  table=str.maketrans(\",\",string.puntuation)\n",
        "  for img,caps in captions.items():\n",
        "    for i,img_caption in enumerate(caps):\n",
        "      img_caption.replace(\"-\",\" \")\n",
        "      desc = img_caption.split()\n",
        "      #converts to lowercase\n",
        "      desc = [word.lower() for word in desc]\n",
        "      #remove punctuation from each token\n",
        "      desc = [word.translate(table) for word in desc]\n",
        "      #remove hanging 's and a \n",
        "      desc = [word for word in desc if(len(word)>1)]\n",
        "      #remove tokens with numbers in them\n",
        "      desc = [word for word in desc if(word.isalpha())]\n",
        "      #convert back to string\n",
        "      img_caption = ' '.join(desc)\n",
        "      captions[img][i]= img_caption\n",
        "  return captions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idb95XmKfOcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_vocabulary(descriptions):\n",
        "  vocab=set()\n",
        "  for key in descriptions.keys():\n",
        "    [vocab.update(d.split()) for d in descriptions[key]]\n",
        "  \n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c05_JcFKvcNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_descriptions(descriptions,filename):\n",
        "  lines=list()\n",
        "  for key,desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "      lines.append(key + '\\t' + desc)\n",
        "  \n",
        "  data='\\n'.join(lines)\n",
        "  file=open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsOYvvB3wxhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_images=\"/content/drive/My Drive/Colab Notebooks/Flickr8k_text\"\n",
        "dataset_text=\"/content/drive/My Drive/Colab Notebooks/Flickr8k_text\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vDl4NXbxg6h",
        "colab_type": "code",
        "outputId": "c6eac232-e914-4f01-bcf4-fca711aa01f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70mj8Gb0D66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename=dataset_images+'/'+'Flickr8k.token.txt'\n",
        "descriptions=all_image_caption(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k2zacjf2YkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('length of descripiton',len(descriptions))\n",
        "clean_description=cleaning_text(descriptions)\n",
        "\n",
        "\n",
        "vocabulary=text_vocabulary(clean_description)\n",
        "save_descriptions(clean_description,\"descriptions.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5QhVo8670r-",
        "colab_type": "text"
      },
      "source": [
        "Extracting Features from the dataset_images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smh5xdUp8EuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_feature(directory):\n",
        "  model=Xception(include_top=False,pooling='avg')\n",
        "  features={}\n",
        "  for img in tqdm(os.listdir(directory)):\n",
        "    filename=directory+'/'+img\n",
        "    image=Image.open(filename)\n",
        "    image=image.resize((299,299))\n",
        "    image=np.expand_dims(image,axis=0)\n",
        "    image=image/127.5\n",
        "    image=image-1.0\n",
        "\n",
        "    feature=maodel.predict(image)\n",
        "    features[img]=feature\n",
        "\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5QLYOWc-mOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features=extract_feature(dataset_images)\n",
        "dump(features,open(\"features.p\",\"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC4hf4ML_goC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features=load(open(\"features.p\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUMv8BQi_wv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the data \n",
        "def load_photos(filename):\n",
        "    file = load_doc(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "\n",
        "def load_clean_descriptions(filename, photos): \n",
        "    #loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "        image, image_caption = words[0], words[1:]\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "def load_features(photos):\n",
        "    #loading all features\n",
        "    all_features = load(open(\"features.p\",\"rb\"))\n",
        "    #selecting only needed features\n",
        "    features = {k:all_features[k] for k in photos}\n",
        "    return features\n",
        "\n",
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "#train = loading_data(filename)\n",
        "train_imgs = load_photos(filename)\n",
        "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlZdrm50Cl1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the data for images\n",
        "\n",
        "def load_photos(filename):\n",
        "  file=load_doc(filename)\n",
        "  photos=file.split(\"\\n\")[:-1]\n",
        "  return photos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crOyhKi5D3i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_clean_description(filename,photos):\n",
        "  file=load_doc(filename)\n",
        "  descriptions={}\n",
        "  for line in file.split(\"\\n\"):\n",
        "    words=lines.split()\n",
        "    if len(words)<1:\n",
        "      continue\n",
        "\n",
        "    \n",
        "    image,image_caption=words[0],words[1:]\n",
        "\n",
        "    if image in photos:\n",
        "      if image not in descriptions:\n",
        "        descriptions[image]=[]\n",
        "      desc='<start> '+\" \".join(image_caption)+' <end>'\n",
        "      descriptions[image].append(desc)\n",
        "    \n",
        "  return descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCs_QQXKM-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_features(photos):\n",
        "  all_features=load(open(\"features.p\".\"rb\"))\n",
        "  features = {k:all_features[k] for k in photos}\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJXb2i8QLuNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "\n",
        "train_imgs=load_photos(filename)\n",
        "train_descriptions=load_clean_description(\"descriptions.txt\",train_imgs)\n",
        "train_features=load_features(train_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5XcYfhhUc-M",
        "colab_type": "text"
      },
      "source": [
        "Tokenizing The vocabulary\n",
        "\n",
        "Computers don’t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a “tokenizer.p” pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkmPHhbUkTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dict_to_list(descriptions):\n",
        "  all_desc=[]\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyl1aTKHXNSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDewpoxHXnmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer(descriptions):\n",
        "  desc_list=dict_to_list(descriptions)\n",
        "  tokenizer=Tokenizer()\n",
        "  tokenizer.fit_on_texts(desc_list)\n",
        "  return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UEI73hDYb9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=create_tokenizer(train_descriptions)\n",
        "dump(tokenizer,open(\"tokenizer.p\",\"wb\"))\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB0Br63OaI4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(descriptions):\n",
        "  desc_list=dict_to_list(descriptions)\n",
        "  return max(len(d.split()) for d in desc_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOkhl3WjdR-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len=max_length(descriptions)\n",
        "max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff9np6MYeqy7",
        "colab_type": "text"
      },
      "source": [
        "**Create Data generator**\n",
        "\n",
        "Let us first see how the input and output of our model will look like. To make this task into a supervised learning task, we have to provide input and output to the model for training. We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.\n",
        "\n",
        "The generator will yield the input and output sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlixFMQ2d4cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(descriptions,tokennizer,features,max_length):\n",
        "  while 1:\n",
        "    for key,description_list in descriptions.items():\n",
        "      feature=features[key][0]\n",
        "      for key, description_list in descriptions.items():\n",
        "            #retrieve photo features\n",
        "            feature = features[key][0]\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
        "            yield [[input_image, input_sequence], output_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qE06dxJ5uAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sequence(tokenizer,max_length,desc_list,feature):\n",
        "  X1,X2,y=list(),list(),list()\n",
        "  for desc in desc_list:\n",
        "    seq=tokenizer.text_to_sequence([desc])[0]\n",
        "    for i in range(1,len(seq)):\n",
        "      in_seq,out_seq=seq[i],seq[:i]\n",
        "      in_seq=pad_sequence([in_seq],maxlen=max_length)[0]\n",
        "      out_seq=to_categorical([out_seq],num_classes=vocab_size)[0]\n",
        "      X1.append(feature)\n",
        "      X2.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "  return np.array(X1),np.array(X2),np.array(y)\n",
        "\n",
        "[a,b],c=next(data_generator(descriptions,tokenizer,features,max_length))\n",
        "\n",
        "a.shape ,b.shape,c.shape()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNBBS3Xr9tyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "def define_model(vocab_size,max_length):\n",
        "  inputs1=Input(shape=(2048,))\n",
        "  fe1=Dropout(0.5)(inputs1)\n",
        "  fe2=Dense(256,activation='relu')(fe1)\n",
        "\n",
        "  inputs2=Input(shape=(max_length,))\n",
        "  se1=Embedding(vocab_size,256,mark_zero=True)(inputs2)\n",
        "  se2=Dropout(0.5)(se1)\n",
        "  se3=LSTM(256)(se2)\n",
        "\n",
        "\n",
        "  decoder1=add(fe1,se3)\n",
        "  decoder2=Dense(256,activation='relu')(decoder1)\n",
        "  decoder3=Dense(vocab_size,ativtation='softmax')(decoder2)\n",
        "\n",
        "  model=Model(inputs=(inputs1,inputs2),outputs=decoder3)\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
        "\n",
        "\n",
        "  print(model.summary())\n",
        "  plot_model(model,to_file='model.png',show_shapes=True)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xriU29bxM5li",
        "colab_type": "text"
      },
      "source": [
        "To train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on your system capability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jihCHFDUzVZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train our model\n",
        "print('Dataset: ', len(train_imgs))\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "print('Photos: train=', len(train_features))\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Description Length: ', max_length)\n",
        "\n",
        "\n",
        "\n",
        "model=define_model(vocab_size,max_length)\n",
        "epochs=10\n",
        "steps=len(train_desriptions)\n",
        "os.mkdir(\"models\")\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  generator=data_generator(train_descriptions,train_festures,tokenizer,max_length)\n",
        "  model.fit_generator(generator,epochs=1,steps_per_epochs=steps,verbose=1)\n",
        "  model.save(\"models/model_\" + str(i) + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}